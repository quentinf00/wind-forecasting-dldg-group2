# -*- coding: utf-8 -*-
"""Group2_amédée_perrine.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fqtCfEh9uEdUVwuMLkcwnnPwUZNscdom
"""

import torch
import torch.nn as nn
import einops
import numpy as np
import xarray as xr

from pathlib import Path
import pandas as pd
import xarray as xr
import torch
from functools import lru_cache
import torch.utils.data as td

class WindForecastDataset(td.Dataset):                                                                              
    def __init__(                                                                                                   
        self,                                                                                                       
        period,                                                                                                     
        hr_path='high_res_data',                                                                                    
        mr_path='med_res_data',                                                                                          
        lr_path='low_res_data',                                                                                          
        out_path='output_data'                                                                                      
    ):                                                                                                              
                                                                                                                    
                                                                                                                    
        self.ds_hr = xr.open_zarr(hr_path)                                                                  
        self.ds_mr = xr.open_zarr(mr_path)                                                                   
        self.ds_lr = xr.open_zarr(lr_path)                                                                   
        self.ds_out = xr.open_zarr(out_path)       
        valid_dates =   (
            self.ds_out.assign(date=lambda ds: ds.valid_time.dt.date.compute())
            .groupby('date')
            .apply(lambda g: np.all(~np.isnan(g.du)))
            .pipe(lambda ds: ds.isel(date=ds))
            .pipe(lambda ds: pd.to_datetime(ds.date.values))
        )                                                     
                                                                                                                    
        self.date_items = sorted(list(set(pd.to_datetime(self.ds_hr.training_item.values, unit='D')) & set(period) & set(valid_dates)))
        self.ds_out
        print(f'{len(self.date_items)} dates available out of {len(period)} dates asked')                           
                                                                                                                    
    def __len__(self):                                                                                              
        return len(self.date_items)                                                                                 

    def check_dim(self, ds):
      if ds.dims.get('training_item', 0) > 1:
        return ds.isel(training_item=0)
      return ds

    def fix_na_mr(self, ds):
      mr_grid = {'g_lat': np.arange(43, 45, 0.05), 'g_lon': np.arange(4, 7.55, 0.05)}

      return (
          ds
            .sel(mr_grid, method='nearest')
            .interpolate_na('g_lat')
            .interpolate_na('g_lon')
      )


    def fix_na_lr(self, ds):
      lr_grid = {'g_lat': np.arange(43, 45, 0.1), 'g_lon': np.arange(4, 7.6, 0.1)}

      return (
          ds
            .sel(lr_grid, method='nearest')
            .interpolate_na('g_lat')
            .interpolate_na('g_lon')
      )
                                                                           
    def __getitem__(self, item):                                                                                    
        date = self.date_items[item]                                                                                
        hr_item = self.ds_hr.sel(training_item=date).pipe(self.check_dim)[['u10', 'v10', 'du', 'dv']].compute().to_array().data.astype('float32')        
        mr_item = self.ds_mr.sel(training_item=date).pipe(self.check_dim)[['u10', 'v10', 'du', 'dv']].pipe(self.fix_na_mr).compute().to_array().data.astype('float32')
        lr_item = self.ds_lr.sel(training_item=date).pipe(self.check_dim)[['u10', 'v10', 'du', 'dv']].pipe(self.fix_na_lr).compute().to_array().data.astype('float32')
        out_item = self.ds_out.sel(dt=str(date.date()))[['du', 'dv']].compute().to_array().data.astype('float32')
        ret_values = (hr_item, mr_item, lr_item, out_item)
        #print(item, date)
        #for r in ret_values:
          #print(r.shape)                                                                                   
        return ret_values

folder = '/homes/q20febvr/local/dldg/windforecast'
hr_path = Path(folder) / 'high_res_data'
mr_path = Path(folder) / 'med_res_data'
lr_path = Path(folder) / 'low_res_data'
out_path = Path(folder) / 'output_data'

test_period = pd.to_datetime('2018-01-01'), pd.to_datetime('2018-12-31') 
train_period = pd.to_datetime('2016-01-01'), test_period[0]                       
                                                                                           

train_ds, val_ds = td.random_split(
    WindForecastDataset(
      pd.to_datetime(pd.date_range(*train_period)),
      hr_path=hr_path,                                                                                    
      mr_path=mr_path,                                                                                          
      lr_path=lr_path,                                                                                          
      out_path=out_path,   
  ),
  [498, 100], generator=torch.Generator().manual_seed(42))

test_ds = WindForecastDataset(
    pd.to_datetime(pd.date_range(*test_period)),
    hr_path=hr_path,                                                                                    
    mr_path=mr_path,                                                                                          
    lr_path=lr_path,                                                                                          
    out_path=out_path, 
  )
                                                                                                                     

bs = 5
train_dl = td.DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True, num_workers=16)
val_dl = td.DataLoader(val_ds, batch_size=bs, shuffle=False, drop_last=True, num_workers=6)
test_dl = td.DataLoader(test_ds, batch_size=bs, shuffle=False,drop_last=True, num_workers=6)

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/


import einops
import torch
import pytorch_lightning as pl
import torch.nn.functional as F
import numpy as np

"""# First Convolutional Network"""

x_hr, x_mr, x_lr, y = next(iter(train_dl))

x_hr.shape

class ConvLSTM(pl.LightningModule):
  def __init__(self):
    super(ConvLSTM, self).__init__()

    self.cnn_high = nn.Sequential(
        nn.Conv2d(4, 8, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(8),
        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),

        nn.Conv2d(8, 16, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(16),
        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),

        nn.Conv2d(16, 32, kernel_size=2, stride=2, padding=0),
        nn.BatchNorm2d(32),
        nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        )

    self.lstm_high = nn.LSTM(input_size=64, hidden_size=80, batch_first=True)
    
    self.dense_high = nn.Sequential(
        nn.Linear(80, 20),
        nn.Linear(20, 10),
        nn.Linear(10, 2)
    )
    
      
    self.cnn_medium = nn.Sequential(
         nn.Conv2d(4, 8, kernel_size=4, stride=1, padding=0),
         nn.BatchNorm2d(8),
         nn.MaxPool2d(kernel_size=4, stride=2, padding=0),

         nn.Conv2d(8, 16, kernel_size=4, stride=1, padding=0),
         nn.BatchNorm2d(16),
         nn.MaxPool2d(kernel_size=4, stride=2, padding=0),

         nn.Conv2d(16, 32, kernel_size=2, stride=2, padding=0),
         nn.BatchNorm2d(32),
         nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
         )
    
    self.lstm_medium = nn.LSTM(input_size=96, hidden_size=80, batch_first=True)
    
    self.dense_medium = nn.Sequential(
         nn.Linear(80, 20),
         nn.Linear(20, 10),
         nn.Linear(10, 2)
     )
    
    self.cnn_low = nn.Sequential(
         nn.Conv2d(4, 8, kernel_size=2, stride=1, padding=0),
         nn.BatchNorm2d(8),
         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),

         nn.Conv2d(8, 16, kernel_size=2, stride=1, padding=0),
         nn.BatchNorm2d(16),
         nn.MaxPool2d(kernel_size=2, stride=2, padding=0),

         nn.Conv2d(16, 32, kernel_size=2, stride=1, padding=0),
         nn.BatchNorm2d(32),
         nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
         )
    
    self.lstm_low = nn.LSTM(input_size=96, hidden_size=80, batch_first=True)

    self.dense_low = nn.Sequential(
         nn.Linear(80, 20),
         nn.Linear(20, 10),
         nn.Linear(10, 2)
     )
    self.test_dates = None
    self.test_figs = {}
    self.y_pred_all = None
    self.y_true_all = None
    self.test_xrds = None
  
    
  def forward(self, x_hr, x_mr, x_lr):
    ## HIGH RESOLUTION
    # convolutions step
    out_high = torch.reshape(x_hr, (bs*x_hr.shape[2], channels, x_hr.shape[3], x_hr.shape[4]))

    out_high = self.cnn_high(out_high)

    out_high = torch.reshape(out_high, (bs, 60, out_high.shape[1]*out_high.shape[2]*out_high.shape[3]))

    # recurrent step
    out_high, _ = self.lstm_high(out_high)
    
    out_high = self.dense_high(out_high)
    
    ## MEDIUM RESOLUTION
    # convolutions step
    out_medium = torch.reshape(x_mr, (bs*x_mr.shape[2], channels, x_mr.shape[3], x_mr.shape[4]))
    out_medium = self.cnn_medium(out_medium)
    out_medium = torch.reshape(out_medium, (bs, 24, out_medium.shape[1]*out_medium.shape[2]*out_medium.shape[3]))
    # # recurrent step
    out_medium, _ = self.lstm_medium(out_medium)

    out_medium = self.dense_medium(out_medium[:,-1,:])
    out_medium = out_medium.unsqueeze(1)
    
    # ## LOW RESOLUTION
    # # convolutions step
    out_low = torch.reshape(x_lr, (bs*x_lr.shape[2], channels, x_lr.shape[3], x_lr.shape[4]))
    out_low = self.cnn_low(out_low)
    out_low = torch.reshape(out_low, (bs, 12, out_low.shape[1]*out_low.shape[2]*out_low.shape[3]))
    
    # # recurrent step
    out_low, _ = self.lstm_low(out_low)

    # # dense step
    out_low = self.dense_low(out_low[:,-1,:])
    out_low = out_low.unsqueeze(1)
    
    #return (out_high, out_medium, out_low), torch.transpose(out_high + out_medium + out_low, 1, 2)
    return torch.transpose(out_high + out_medium + out_low, 1, 2)

  def training_step(self, batch, batch_idx):
    x_hr, x_mr, x_lr, y = batch
    y_pred = self(x_hr,x_mr,x_lr)
    loss = F.mse_loss(y_pred, y)
    # Logging to TensorBoard by default
    self.log("train_loss", loss)
    return loss

  def validation_step(self, batch, batch_idx):
    x_hr, x_mr, x_lr, y = batch
    y_pred = self(x_hr,x_mr,x_lr)
    loss = F.mse_loss(y_pred,y)
    # Logging to TensorBoard by default
    self.log("validation_loss", loss)
    return loss

  def on_test_epoch_start(self):
    self.test_dates = self.trainer.test_dataloaders[0].dataset.date_items
    self.test_dates = self.test_dates[: len(self.test_dates) // bs * bs]

  def test_step(self, batch, batch_idx):
    x_hr, x_mr, x_lr, y = batch
    y_pred = self(x_hr,x_mr,x_lr)

    loss = F.mse_loss(y_pred,y)
    self.log("test_loss", loss)
    return y_pred.cpu().numpy(), y.cpu().numpy() 

  def test_epoch_end(self, test_outputs):
    self.y_pred_all = np.concatenate([it[0] for it in test_outputs])
    self.y_true_all = np.concatenate([it[1] for it in test_outputs])

    self.test_xrds = xr.Dataset(
        {
            'y_true': (('date', 'var', 'time'), self.y_true_all),
            'y_pred': (('date', 'var', 'time'), self.y_pred_all),
        },
        coords={
            'date':  (('date',), self.test_dates), ## Watch out the indexes correspond to the number of days (i.e. limite test batches)
            'var': (('var',), ['du', 'dv']),
            'time': (('time',), np.arange(0, 6*60, 6)),
        }
    )


    amp_error = (
      self.test_xrds
      .assign(pred_err=lambda ds: ds.y_true - ds.y_pred)
      [['y_true', 'pred_err']]
      .pipe(
          lambda ds: ds.sel(var='du')**2 + ds.sel(var='dv')**2
      ).pipe(np.sqrt)
    )

    fig, ax = plt.subplots()
    amp_error.mean('time').y_true.plot(ax=ax)
    amp_error.mean('time').pred_err.plot(ax=ax)
    ax.legend(['AROME', 'ours'])
    self.test_figs['amp err'] = fig
    self.logger.experiment.add_figure('amplitude err ts', fig, global_step=self.current_epoch)

    fig, ax = plt.subplots()
    amp_error.mean('date').y_true.plot(ax=ax)
    amp_error.mean('date').pred_err.plot(ax=ax)
    ax.legend(['AROME', 'ours'])
    self.test_figs['win_err'] = fig
    self.logger.experiment.add_figure('amplitude err win', fig, global_step=self.current_epoch)


    # % improvement
    percent_improvement = (
        self.test_xrds
        .assign(pred_err=lambda ds: ds.y_true - ds.y_pred)
        .pipe(np.abs)
        .mean(['time', 'var'])
        .assign(ratio=lambda ds: ds.pred_err / ds.y_true)
    )

    mean_improv = percent_improvement.ratio.mean().item()
    worst_improv = percent_improvement.ratio.max().item()
    best_improv = percent_improvement.ratio.min().item()
    self.log('mean_improvement', mean_improv)
    self.log('worst_improv', worst_improv)
    self.log('best_improv', best_improv)

    fig, ax = plt.subplots()
    percent_improvement.ratio.plot(ax=ax)
    self.logger.experiment.add_figure('Ratio TS', fig, global_step=self.current_epoch)

  def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)
    return optimizer

net = ConvLSTM()
channels=4

y = net(x_hr, x_mr, x_lr)

y.shape
mc = pl.callbacks.ModelCheckpoint(monitor='validation_loss')
trainer = pl.Trainer(gpus=1, max_epochs=50, log_every_n_steps=1, progress_bar_refresh_rate=1)
lit_mod = ConvLSTM()
# trainer.fit(lit_mod, train_dl,val_dataloaders=val_dl)

import matplotlib.pyplot as plt
lit_mod = ConvLSTM.load_from_checkpoint('ckpt_perrine.ckpt')
trainer = pl.Trainer(gpus=1, max_epochs=10, log_every_n_steps=1, progress_bar_refresh_rate=1)
trainer.test(lit_mod, test_dataloaders=test_dl)

# self = lit_mod
# test_xrds = xr.Dataset(
#           {
#               'y_true': (('date', 'var', 'time'), self.y_true_all),
#               'y_pred': (('date', 'var', 'time'), self.y_pred_all),
#           },
#           coords={
#               'date':  (('date',), self.test_dates[:10]),
#               'var': (('var',), ['du', 'dv']),
#               'time': (('time',), np.arange(0, 6*60, 6)),
#           }
#       )
# print(test_xrds)

# percent_improvement = (
#     test_xrds
#     .assign(pred_err=lambda ds: ds.y_true - ds.y_pred)
#     .pipe(np.abs)
#     .mean(['time', 'var'])
#     .assign(ratio=lambda ds: ds.pred_err / ds.y_true)
# )

# mean_improv = percent_improvement.ratio.mean()
# worst_improv = percent_improvement.ratio.max()
# best_improv = percent_improvement.ratio.min()

# fig, ax = plt.subplots()
# percent_improvement.ratio.plot(ax=ax)
